<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[thesis-citrap]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>site-lib/media/favicon.png</url><title>thesis-citrap</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 11 Aug 2025 12:26:27 GMT</lastBuildDate><atom:link href="site-lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 11 Aug 2025 12:26:23 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[Drug Similarity]]></title><description><![CDATA[Problems: How can we compute the similarity of two drugs and . If is a generic name and is a brand name, how can we calculate similarity? Especially when is very different from (as with "Advil" and "Ibuprofen"). Why bother?
Drug prescribing errors due to look-alike and sound-alike (LASA) persists <a data-tooltip-position="top" aria-label="https://qualitysafety.bmj.com/content/qhc/28/11/908.full.pdf" rel="noopener nofollow" class="external-link is-unresolved" href="https://qualitysafety.bmj.com/content/qhc/28/11/908.full.pdf" target="_self">LGL+19</a>.
This becomes a pavement to explore string similarity algorithms with modern transformers (BERT) as assistants.
Title: A hybrid approach for drug similarity computationSince the problem is with both look-alike (orthographic) and sound-alike (phonetic) we can use similarity algorithms for both orthographic and phonetic strings.
To work on, maybe we need to do a separate literature on this. <br>Phonetic Alignment due to <a data-tooltip-position="top" aria-label="https://webdocs.cs.ualberta.ca/~kondrak/papers/chum.pdf" rel="noopener nofollow" class="external-link is-unresolved" href="https://webdocs.cs.ualberta.ca/~kondrak/papers/chum.pdf" target="_self">Kon03</a>. To add, maybe we need to do a separate literature review on this.
Let be a mapping from a set of pharmaceutical identifiers to an -dimensional continuous vector space, where for some is a dense, real-valued embedding representation as produced by some language model .Then the cosine similarity of two word embeddings and of two drugs and is given byFurthermore, <br>
The language model we talk about here is <a data-tooltip-position="top" aria-label="Drug Similarity > Issue what about brand names?" data-href="Drug Similarity#Issue what about brand names?" href="ideas/drug-similarity.html#Issue_what_about_brand_names?_0" class="internal-link" target="_self" rel="noopener nofollow">BERT</a>.
Looking at Advil and Ibuprofen, how can we determine that these two are the same?<br>
<a data-tooltip-position="top" aria-label="[SC16] Characteristics That May Help in the Identification of Potentially Confusing Proprietary Drug Names.pdf" data-href="[SC16] Characteristics That May Help in the Identification of Potentially Confusing Proprietary Drug Names.pdf" href="library/[sc16]-characteristics-that-may-help-in-the-identification-of-potentially-confusing-proprietary-drug-names.html" class="internal-link" target="_self" rel="noopener nofollow">SC16</a>.We need a domain-specific language model that specifically looks at drug named and other pharmaceutical words.<br>Thankfully <a data-tooltip-position="top" aria-label="https://academic.oup.com/bib/article/24/4/bbad226/7197744" rel="noopener nofollow" class="external-link is-unresolved" href="https://academic.oup.com/bib/article/24/4/bbad226/7197744" target="_self">VSR+23</a> with their <a data-tooltip-position="top" aria-label="https://huggingface.co/Lianglab" rel="noopener nofollow" class="external-link is-unresolved" href="https://huggingface.co/Lianglab" target="_self">PharmBERT</a> BERT model provides a recent solution.
Since this BERT model is only recent, this adds to the motivation for pursuing this research.
We identify the gap:
<br>Existing LASA-based methods (<a data-tooltip-position="top" aria-label="[KD05] Automatic identification of confusable drug names.pdf" data-href="[KD05] Automatic identification of confusable drug names.pdf" href="library/[kd05]-automatic-identification-of-confusable-drug-names.html" class="internal-link" target="_self" rel="noopener nofollow"></a>) do not leverage recent advances in transformer-based language models.
PharmBERT by VSR+23 is not systematically evaluated for LASA detection or brand–generic matching.
Brand and generic names may be less represented, limiting the effectiveness of pure embedding-based similarity.
Many countries or institutions lack large, curated biomedical corpora or rich drug databases (e.g., RxNorm, DrugBank) needed to train or fine-tune domain-specific language models like PharmBERT.
Putting it all together we can create a hybrid method for drug similarity?
If two drugs and are very similar to one another (i.e., acetaminophen and ibuprofen) it's not necessary to use BERT.
<br>On the other hand, if one is a <a data-tooltip-position="top" aria-label="Drug Similarity > Issue what about brand names?" data-href="Drug Similarity#Issue what about brand names?" href="ideas/drug-similarity.html#Issue_what_about_brand_names?_0" class="internal-link" target="_self" rel="noopener nofollow">brand name</a> or the two are very different from each other. Then, a cosine similarity may be necessary. ]]></description><link>ideas/drug-similarity.html</link><guid isPermaLink="false">ideas/Drug Similarity.md</guid><pubDate>Mon, 11 Aug 2025 12:24:56 GMT</pubDate></item><item><title><![CDATA[Ideas]]></title><description><![CDATA[ Read this first!Looking at literature there seems to be two flavors of NLP:
Machine Learning.
Rule-based Systems.
When to go (1), when we have a lot of corpora or datasets to munch through.When to go (2), when we need efficiency and the language in question is a low-resource language (a language with limited linguistic resources or data).
Tagalog (and majority of the Philippine languages) are low-resource.
Majority of NLP research has its focus on machine learning, neural networks, etc. thus isolating those languages that are low-resource or people who do not have the capacity to train those.
Motivation and Background
Neural networks are not transparent (we don't really know what the neural network knows). Hence, if we have a neural network trained over an alphabet we can extract what that neural network knows by constructing a DFA over .
This extraction is done by Angluin's L* Algorithm and a minimally adequate teacher.
Idea: By training a Transformer from Tagalog corpora and an oracle , we may extract all the possible sentences knows as the DFA .Scrapped... why?
Only works on regular languages. And, some Tagalog (or majority of human-languages) have some non-regular grammar rules. Maybe we can take a subset of grammars and check if this actually works. References:
<a data-tooltip-position="top" aria-label="[Ang87] Learning Regular Sets from Queries and Counterexamples.pdf" data-href="[Ang87] Learning Regular Sets from Queries and Counterexamples.pdf" href="library/[ang87]-learning-regular-sets-from-queries-and-counterexamples.html" class="internal-link" target="_self" rel="noopener nofollow">Ang87</a>, <a data-tooltip-position="top" aria-label="[GS22] Finite-State Text Processing.pdf" data-href="[GS22] Finite-State Text Processing.pdf" href="library/[gs22]-finite-state-text-processing.html" class="internal-link" target="_self" rel="noopener nofollow">GS22</a>, <a data-tooltip-position="top" aria-label="[Kle51] Representation of Events in Nerve Nets and Finite Automata.pdf" data-href="[Kle51] Representation of Events in Nerve Nets and Finite Automata.pdf" href="library/[kle51]-representation-of-events-in-nerve-nets-and-finite-automata.html" class="internal-link" target="_self" rel="noopener nofollow">Kle51</a>, <a data-tooltip-position="top" aria-label="[WGY20] Extracting Automata from Recurrent Neural Networks.pdf" data-href="[WGY20] Extracting Automata from Recurrent Neural Networks.pdf" href="library/[wgy20]-extracting-automata-from-recurrent-neural-networks.html" class="internal-link" target="_self" rel="noopener nofollow">WGY20</a>, <a data-tooltip-position="top" aria-label="[ZWS24] Automata Extraction from Transformers.pdf" data-href="[ZWS24] Automata Extraction from Transformers.pdf" href="library/[zws24]-automata-extraction-from-transformers.html" class="internal-link" target="_self" rel="noopener nofollow">ZWS24</a>.<br>Long version: <a data-href="Automata Extraction" href="ideas/automata-extraction.html" class="internal-link" target="_self" rel="noopener nofollow">Automata Extraction</a>This canvas and directory contains all ideas we have come up with.]]></description><link>ideas/ideas.html</link><guid isPermaLink="false">ideas/Ideas.canvas</guid><pubDate>Thu, 07 Aug 2025 10:11:32 GMT</pubDate></item><item><title><![CDATA[Index]]></title><description><![CDATA[Research ideas and word blabbing in Natural Language Processing (NLP).
NLP (in general): <a data-tooltip-position="top" aria-label="[JM23] Speech and Language Processing.pdf" data-href="[JM23] Speech and Language Processing.pdf" href="library/[jm23]-speech-and-language-processing.html" class="internal-link" target="_self" rel="noopener nofollow">JM23</a>
<br>Rule-based Systems: <a data-href="[GS22] Finite-State Text Processing.pdf" href="library/[gs22]-finite-state-text-processing.html" class="internal-link" target="_self" rel="noopener nofollow">[GS22] Finite-State Text Processing.pdf</a>
<br><a data-tooltip-position="top" aria-label="https://aclanthology.org/" rel="noopener nofollow" class="external-link is-unresolved" href="https://aclanthology.org/" target="_self">ACL Anthology</a>
<br><a data-tooltip-position="top" aria-label="https://arxiv.org/list/cs.CL/recent" rel="noopener nofollow" class="external-link is-unresolved" href="https://arxiv.org/list/cs.CL/recent" target="_self">arXiv &gt; cs.CL</a> Machine Translator for Bikol or Cebuano
Speech to Text (or Speech to Text) for Bikol or Cebuano
Spelling or Grammar Checker for other Philippine Languages
Computing Drug Similarity
NLP Toolkit for Filipino and Philippine Languages
Other related topics
<br><a data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="🏠" aria-label="🏠" data-icon="🏠" aria-hidden="true" style="transform: translateY(0px);"></span>Index</a>🏠 (this one)
Contains the gist of the entire document<br>
<a data-href="Library" href="library/library.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="📚" aria-label="📚" data-icon="📚" aria-hidden="true" style="transform: translateY(0px);"></span>Library</a>📚
This directory contains all references (used and unused) for the entire knowledge base. Just keep adding stuff papers, books, and articles here (even if we're going to end up not using it).<br>
<a data-tooltip-position="top" aria-label="Ideas.canvas" data-href="Ideas.canvas" href="ideas/ideas.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="💡" aria-label="💡" data-icon="💡" aria-hidden="true" style="transform: translateY(0px);"></span>Ideas</a>💡
This canvas and directory contains all ideas we have come up with.]]></description><link>index.html</link><guid isPermaLink="false">Index.md</guid><pubDate>Mon, 11 Aug 2025 12:26:26 GMT</pubDate></item><item><title><![CDATA[README]]></title><description><![CDATA[Knowledge base for research. The home page is at <a data-href="Index" href="index.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="🏠" aria-label="🏠" data-icon="🏠" aria-hidden="true" style="transform: translateY(0px);"></span>Index</a>🏠.]]></description><link>readme.html</link><guid isPermaLink="false">README.md</guid><pubDate>Thu, 07 Aug 2025 09:16:15 GMT</pubDate></item><item><title><![CDATA[Automata Extraction]]></title><description><![CDATA[This idea is scrapped
Automata Extraction only works on regular languages, since Philippine grammar is non-regular (it is safe to say that majority, if not all, human languages are non-regular). Then we cannot use Automata Extraction for a neural network on a Philippine language.
Work started with <a data-tooltip-position="top" aria-label="[kle51]-representation-of-events-in-nerve-nets-and-finite-automata.pdf" data-href="[kle51]-representation-of-events-in-nerve-nets-and-finite-automata.pdf" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Kle51</a> and recently worked on by <a data-tooltip-position="top" aria-label="[wgy20]-extracting-automata-from-recurrent-neural-networks.pdf" data-href="[wgy20]-extracting-automata-from-recurrent-neural-networks.pdf" href=".html" class="internal-link" target="_self" rel="noopener nofollow">WGY20</a>.<br>DFA Extraction is a procedure of extracting a DFA over some alphabet from a Recurrent Neural Networks (RNNs) trained over . We can say that is extracted from if They do this by Exact Learning (by Angluin's L* Algorithm <a data-tooltip-position="top" aria-label="[Ang87] Learning Regular Sets from Queries and Counterexamples.pdf" data-href="[Ang87] Learning Regular Sets from Queries and Counterexamples.pdf" href="library/[ang87]-learning-regular-sets-from-queries-and-counterexamples.html" class="internal-link" target="_self" rel="noopener nofollow">Ang87</a>), we have an oracle<a data-footref="1" href="#fn-1-bb297242003193b3" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a> that can answer two types of questions:
Membership Query (MQ): "Does accept the string ?"
Equivalence Query (EQ): "Is correct? If not, provide a counterexample." Here counterexample is some string that cannot be produced by . Notice that the constraint above uses universal quantification (), hence we can disprove the equivalence of and by counterexample.
We start with a hypothesis DFA (the subscript 0 denotes the first iteration). We ask one of the questions to the oracle and refine depending on its response.
For MQs and for some string , if the oracle says that then update (this new DFA is , or generally ) such that .
For EQs and for some "version" , if the oracle returns an , that means . Then, update to such that .
Motivation
Neural networks abstract a lot of things; if we train an RNN over a language this does not tell us much about . On the other hand, creating a DFA over and ensuring that the RNN knows . We also get a "concrete" model of without any form of abstractions. <br>More recent research <a data-tooltip-position="top" aria-label="[zws24]-automata-extraction-from-transformers.pdf" data-href="[zws24]-automata-extraction-from-transformers.pdf" href=".html" class="internal-link" target="_self" rel="noopener nofollow">ZWS24</a> builds upon WGY20 and uses the more recent Transformer architecture (think of the T in GPT). <br>We call this oracle a Minimally Adequate Teacher (MAT), one that correctly answer MQs and EQs.<a href="#fnref-1-bb297242003193b3" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow">↩︎</a>
]]></description><link>ideas/automata-extraction.html</link><guid isPermaLink="false">ideas/Automata Extraction.md</guid><pubDate>Thu, 07 Aug 2025 09:15:09 GMT</pubDate></item><item><title><![CDATA[automata-extraction]]></title><link>ideas/automata-extraction.html</link><guid isPermaLink="false">ideas/automata-extraction.html</guid><pubDate>Thu, 07 Aug 2025 09:12:04 GMT</pubDate></item><item><title><![CDATA[ideas]]></title><link>ideas/ideas.html</link><guid isPermaLink="false">ideas/ideas.html</guid><pubDate>Thu, 07 Aug 2025 09:11:43 GMT</pubDate></item></channel></rss>